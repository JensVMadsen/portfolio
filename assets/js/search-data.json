{
  
    
        "post0": {
            "title": "Title",
            "content": "Introduction . This whole notebook should be run (&lt;2min) to get inputs and outputs for modelling, mapping and decoding. . import time, ipywidgets, pickle, os import numpy as np import pandas as pd from itertools import combinations, product from scipy import stats, interpolate import helper_functions as HF import seaborn as sns import matplotlib.pyplot as plt #pd.set_option(&#39;display.max_rows&#39;, None) #pd.set_option(&#39;display.max_columns&#39;, None) np.seterr(divide=&#39;ignore&#39;) pass . Wind farm info . This snippets shows some info about the different sites . X_info = pd.read_excel(&#39;data/project_data.xlsx&#39;, sheet_name=&#39;site info DMI&#39;) X_info . Name Country lat long WTG type No WTG No blades Inspections Weather ID Vin Vr Vout RPM_min RPM_max Radius Hub height wsp scaler Lvl height . 0 Brorstrup | DK | 56.7661 | 9.6134 | S120/3600 | 2 | 6 | 2012-01-01,2019-05-01 | 1 | 3.5 | 14.0 | 25 | 5.0 | 13.0 | 60 | 88 | 1 | 83.2 | . 1 Løgtved | DK | 55.6765 | 11.2743 | V100/2000 | 3 | 9 | 2016-06-01,2018-05-01,2019-05-01 | 2 | 3.5 | 12.0 | 22 | 7.0 | 13.4 | 50 | 78 | 1 | 83.2 | . 2 Tween Bridge | GB | 53.6666 | -0.8899 | V90/3000 | 22 | 66 | 2012-03-01,2017-09-01 | 4 | 3.0 | 13.5 | 25 | 8.2 | 17.3 | 45 | 80 | 1 | 83.2 | . 3 Märkische Heide | DE | 51.9688 | 13.9799 | V90/2000 | 8 | 24 | 2009-05-01,2016-05-01,2019-05-01 | 6 | 3.0 | 13.5 | 25 | 8.2 | 17.3 | 45 | 125 | 1 | 136.0 | . 4 Princess Amalia | NL | 52.5900 | 4.2200 | V80/2000 | 60 | 180 | 2013-05-01,2018-05-01,2019-05-01,2020-05-01 | 7 | 3.5 | 14.5 | 25 | 9.0 | 19.0 | 40 | 59 | 1 | 58.7 | . 5 Solberg | SW | 63.7994 | 17.3943 | V126/3450 | 22 | 66 | 2018-01-01,2019-05-01,2020-05-01 | 49 | 4.5 | 11.5 | 22 | 5.0 | 13.0 | 63 | 117 | 1 | 108.8 | . 6 Camster | GB | 58.4060 | -3.2624 | V80/2000 | 25 | 75 | 2013-07-01,2018-05-01 | 10 | 3.5 | 14.5 | 25 | 9.0 | 19.0 | 40 | 78 | 1 | 83.2 | . Weather data stratification . We need to stratify our weather data so we have one input for inspection period. This code prepares the weather data for each sample based on the inspection dates . X_tt = pd.DataFrame() # loop through all wind farms for idx, row in X_info.iterrows(): # load weather data for each wind farm based on weather file ID df = pd.read_csv(&#39;data/training/WF&#39;+str(row[&#39;Weather ID&#39;])+&#39;_precip_WS_WD_extended_HHupdated.csv&#39;) # convert time column to datetime format try: df[&#39;time&#39;] = pd.to_datetime(df[&#39;Unnamed: 0&#39;]) except: df[&#39;time&#39;] = pd.to_datetime(df[&#39;time&#39;]) # set time column as index df.set_index(&#39;time&#39;,inplace=True) # loop through all unique periods for i in list(combinations(row[&#39;Inspections&#39;].split(&#39;,&#39;), 2)): # prepare inputs using the helper function cols = HF.prepare_inputs(df, i, row, z_lvl=row[&#39;Lvl height&#39;], tbi=3600) # append inputs to dataframe X_tt = X_tt.append(cols, ignore_index=True) X_tt . WF start date end date months t_scaler prec_sum ws_sum tke_sum prec_mean ws_mean tke_mean impingement da_inc . 0 Brorstrup | 2012-01-01 | 2019-05-01 | 88.0 | 1.26 | 6433.538273 | 461307.060595 | 58078.544265 | 0.128985 | 9.248708 | 1.164412 | 96.005915 | 0.786705 | . 1 Løgtved | 2016-06-01 | 2018-05-01 | 23.0 | 1.00 | 1237.562318 | 143919.129525 | 9064.112082 | 0.075319 | 8.759000 | 0.551647 | 20.541667 | 0.116657 | . 2 Løgtved | 2016-06-01 | 2019-05-01 | 35.0 | 1.00 | 1758.937686 | 210339.803826 | 16799.441743 | 0.070147 | 8.388427 | 0.669968 | 29.199881 | 0.156682 | . 3 Løgtved | 2018-05-01 | 2019-05-01 | 12.0 | 1.00 | 521.375368 | 66430.016633 | 7735.568618 | 0.060309 | 7.684212 | 0.894803 | 8.658213 | 0.040025 | . 4 Tween Bridge | 2012-03-01 | 2017-09-01 | 66.0 | 1.33 | 3523.168521 | 300681.228387 | 35457.630242 | 0.099999 | 8.534322 | 1.006404 | 52.078595 | 0.299888 | . 5 Märkische Heide | 2009-05-01 | 2016-05-01 | 84.0 | 2.49 | 4106.410059 | 415622.383524 | 41458.311973 | 0.170809 | 17.288066 | 1.724484 | 62.552778 | 0.585163 | . 6 Märkische Heide | 2009-05-01 | 2019-05-01 | 120.0 | 1.72 | 5866.043133 | 592357.076532 | 57021.883915 | 0.117617 | 11.877072 | 1.143319 | 90.353759 | 0.829741 | . 7 Märkische Heide | 2016-05-01 | 2019-05-01 | 36.0 | 1.00 | 1759.152528 | 177269.203133 | 16480.181809 | 0.068094 | 6.861857 | 0.637926 | 27.376590 | 0.247092 | . 8 Princess Amalia | 2013-05-01 | 2018-05-01 | 60.0 | 1.04 | 4119.002753 | 383803.786882 | 20797.127892 | 0.100434 | 9.358329 | 0.507099 | 71.582376 | 1.226624 | . 9 Princess Amalia | 2013-05-01 | 2019-05-01 | 72.0 | 1.03 | 4711.439212 | 460751.225546 | 23564.837998 | 0.094991 | 9.289527 | 0.475107 | 82.377549 | 1.416901 | . 10 Princess Amalia | 2013-05-01 | 2020-05-01 | 84.0 | 1.03 | 5620.196770 | 544218.153967 | 27057.615912 | 0.096527 | 9.346973 | 0.464716 | 98.577428 | 1.734760 | . 11 Princess Amalia | 2018-05-01 | 2019-05-01 | 12.0 | 1.00 | 600.571168 | 76958.926446 | 2811.923707 | 0.069931 | 8.961216 | 0.327425 | 10.923373 | 0.192670 | . 12 Princess Amalia | 2018-05-01 | 2020-05-01 | 24.0 | 1.00 | 1506.119020 | 160254.448022 | 6316.396732 | 0.087499 | 9.310082 | 0.366955 | 27.058942 | 0.508389 | . 13 Princess Amalia | 2019-05-01 | 2020-05-01 | 12.0 | 1.00 | 905.548341 | 83297.437347 | 3504.591158 | 0.104979 | 9.656554 | 0.406282 | 16.135584 | 0.315719 | . 14 Solberg | 2018-01-01 | 2019-05-01 | 16.0 | 1.00 | 311.255188 | 78248.213688 | 8899.638372 | 0.028548 | 7.176760 | 0.816256 | 4.644573 | 0.063678 | . 15 Solberg | 2018-01-01 | 2020-05-01 | 28.0 | 1.00 | 774.064942 | 142411.173071 | 16564.841328 | 0.039319 | 7.233767 | 0.841410 | 11.240074 | 0.172157 | . 16 Solberg | 2019-05-01 | 2020-05-01 | 12.0 | 1.00 | 462.809754 | 64172.139547 | 7666.317368 | 0.052682 | 7.304740 | 0.872660 | 6.595501 | 0.108478 | . 17 Camster | 2013-07-01 | 2018-05-01 | 58.0 | 1.05 | 4508.322322 | 331497.860727 | 46444.603144 | 0.112396 | 8.264512 | 1.157902 | 71.520520 | 0.675748 | . Prepare inputs for visual sanity check . This section prepares an input file with a precomputed variable space that can be used as a look-up table for fast physicality checks. . alpha = 0.143 WF_list = [&#39;Princess Amalia&#39;, &#39;Märkische Heide&#39;, &#39;Solberg&#39;] dates = pd.date_range(start=&#39;2012-12-31&#39;, end=&#39;2023-01-01&#39;, freq=&#39;6M&#39;) months = [date_obj.strftime(&#39;%Y-%m-%d&#39;) for date_obj in dates] W0_range = np.arange(0, 0.51, 0.02) counter = 0 results = pd.DataFrame() for idx, row in X_info[X_info[&#39;Name&#39;].isin(WF_list)].iterrows(): df = pd.read_csv(&#39;data/training/WF&#39;+str(row[&#39;Weather ID&#39;])+&#39;_precip_WS_WD_extended_HHupdated.csv&#39;) # convert time column to datetime format try: df[&#39;time&#39;] = pd.to_datetime(df[&#39;Unnamed: 0&#39;]) except: df[&#39;time&#39;] = pd.to_datetime(df[&#39;time&#39;]) # set time column as index df.set_index(&#39;time&#39;,inplace=True) for i in months: cols = HF.prepare_inputs(df, (&#39;2013-01-01&#39;, i), row, z_lvl=row[&#39;Lvl height&#39;], tbi=3600) # DMI for j in W0_range: cols[&#39;W0&#39;] = j cols[&#39;W02&#39;] = j results = results.append(cols, ignore_index=True) counter += 1 print(&#39;Running iteration&#39;, counter, &#39;out of&#39;, len(WF_list)*len(months)*len(W0_range), end=&#39; r&#39;) results.to_csv(&#39;data/sanity_check_inputs_DMI.csv&#39;, index=False) . Running iteration 1638 out of 1638194 out of 1638 242 out of 1638 285 out of 163816381638 768 out of 1638 16381613 out of 1638 . Defect analysis . This section manually loads all the inspection files and organizes the defects so they have common columns and can be merged. . X_raw = pd.DataFrame(columns=[&#39;WF&#39;, &#39;year&#39;, &#39;WTG ID&#39;, &#39;Blade&#39;, &#39;Vertical distance&#39;, &#39;Type&#39;, &#39;Severity&#39;, &#39;Surface area&#39;, &#39;Layer&#39;]) path_name = &#39;data/inspections/&#39; ##### WF1 Brorstrup 2019 ##### folder_name = &#39;Brorstrup 2019/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) df = df.drop_duplicates(subset=[&#39;Defect Id&#39;]) tmp = pd.DataFrame() tmp[&#39;WF&#39;] = df[&#39;Windfarm&#39;].values tmp[&#39;year&#39;] = &#39;2019&#39; tmp[&#39;WTG ID&#39;] = df[&#39;Turbine&#39;].values tmp[&#39;Blade&#39;] = df[&#39;Blade&#39;].values tmp[&#39;Vertical distance&#39;] = df[&#39;Vertical Distance&#39;].values tmp[&#39;Type&#39;] = df[&#39;Defect Type&#39;].values tmp[&#39;Severity&#39;] = df[&#39;Severity&#39;].values tmp[&#39;Surface area&#39;] = df[&#39;Surface Area&#39;].values tmp[&#39;Layer&#39;] = df[&#39;Layer&#39;].values tmp[&#39;maxRPM&#39;] = 13 X_raw = X_raw.append(tmp, ignore_index=True) ##### WF2 Løgtved 2018 ##### folder_name = &#39;Loegtved 2018/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].unique(): tmp = df[df[&#39;Turbine&#39;]==wtg] for blade in tmp[&#39;Blade&#39;].unique(): tmp1 = tmp[tmp[&#39;Blade&#39;]==blade] tmp1 = tmp1.drop_duplicates(subset=[&#39;DefectId&#39;]) tmp2 = pd.DataFrame() tmp2[&#39;Blade&#39;] = tmp1[&#39;Blade&#39;].values tmp2[&#39;WTG ID&#39;] = &#39;WTG 01&#39; tmp2[&#39;year&#39;] = &#39;2018&#39; tmp2[&#39;WF&#39;] = &#39;Løgtved&#39; tmp2[&#39;Vertical distance&#39;] = tmp1[&#39;Vertical_Distance&#39;].values tmp2[&#39;Type&#39;] = tmp1[&#39;DefectType&#39;].values tmp2[&#39;Severity&#39;] = tmp1[&#39;Severity&#39;].values tmp2[&#39;Surface area&#39;] = tmp1[&#39;Side&#39;].values tmp2[&#39;Layer&#39;] = tmp1[&#39;Layer&#39;].values tmp2[&#39;maxRPM&#39;] = 13.4 X_raw = X_raw.append(tmp2, ignore_index=True) ##### WF2 Løgtved 2019 ##### folder_name = &#39;Loegtved 2019/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) df = df.drop_duplicates(subset=[&#39;Defect Id&#39;]) tmp = pd.DataFrame() tmp[&#39;WTG ID&#39;] = df[&#39;Turbine&#39;].values tmp[&#39;year&#39;] = &#39;2019&#39; tmp[&#39;WF&#39;] = &#39;Løgtved&#39; tmp[&#39;Blade&#39;] = df[&#39;Blade&#39;].values tmp[&#39;Vertical distance&#39;] = df[&#39;Vertical Distance&#39;].values tmp[&#39;Type&#39;] = df[&#39;Defect Type&#39;].values tmp[&#39;Severity&#39;] = df[&#39;Severity&#39;].values tmp[&#39;Surface area&#39;] = df[&#39;Surface Area&#39;].values tmp[&#39;Layer&#39;] = df[&#39;Layer&#39;].values tmp[&#39;maxRPM&#39;] = 13.4 X_raw = X_raw.append(tmp, ignore_index=True) ##### WF4 Tween Bridge 2017 ##### folder_name = &#39;Tween Bridge 2017/&#39; fnames = os.listdir(path_name+folder_name) counter = 0 for file in fnames: df = pd.read_excel(path_name+folder_name+file) tmp = pd.DataFrame() if counter == 0: tmp[&#39;WTG ID&#39;] = df[&#39;WTG&#39;].values tmp[&#39;Blade&#39;] = df[&#39;Blade nB-G-R&#39;].values tmp[&#39;Vertical distance&#39;] = df[&#39; Root[m]&#39;].values tmp[&#39;Type&#39;] = df[&#39;Eon Defect type&#39;].values tmp[&#39;Surface area&#39;] = df[&#39; Surface Area&#39;].values elif counter == 6: tmp[&#39;WTG ID&#39;] = df[&#39;Turbine&#39;].values tmp[&#39;Blade&#39;] = df[&#39;Blade&#39;].values tmp[&#39;Vertical distance&#39;] = df[&#39;Vertical_Distance&#39;].values tmp[&#39;Type&#39;] = df[&#39;DefectType&#39;].values tmp[&#39;Surface area&#39;] = df[&#39;Side&#39;].values else: tmp[&#39;WTG ID&#39;] = df[&#39;Turbine&#39;].values tmp[&#39;Blade&#39;] = df[&#39;Blade&#39;].values tmp[&#39;Vertical distance&#39;] = df[&#39;AdjustedDistanceFromRoot&#39;].values tmp[&#39;Type&#39;] = df[&#39;DefectType&#39;].values tmp[&#39;Surface area&#39;] = df[&#39;Side&#39;].values tmp[&#39;year&#39;] = &#39;2017&#39; tmp[&#39;WF&#39;] = &#39;Tween Bridge&#39; tmp[&#39;Severity&#39;] = df[&#39;Severity&#39;].values tmp[&#39;Layer&#39;] = df[&#39;Layer&#39;].values tmp[&#39;maxRPM&#39;] = 17.3 X_raw = X_raw.append(tmp, ignore_index=True) counter += 1 ##### WF6 Märkische Heide 2016 ##### folder_name = &#39;Märkische Heide 2016/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].unique(): tmp = df[df[&#39;Turbine&#39;]==wtg] for blade in tmp[&#39;Blade&#39;].unique(): tmp1 = tmp[tmp[&#39;Blade&#39;]==blade] tmp1 = tmp1.drop_duplicates(subset=[&#39;DefectId&#39;]) tmp2 = pd.DataFrame() tmp2[&#39;WTG ID&#39;] = tmp1[&#39;Turbine&#39;].values.astype(&#39;int32&#39;) tmp2[&#39;year&#39;] = &#39;2016&#39; tmp2[&#39;WF&#39;] = &#39;Märkische Heide&#39; tmp2[&#39;Blade&#39;] = tmp1[&#39;Blade&#39;].values.astype(&#39;int32&#39;) tmp2[&#39;Vertical distance&#39;] = tmp1[&#39;Vertical_Distance&#39;].values tmp2[&#39;Type&#39;] = tmp1[&#39;DefectType&#39;].values tmp2[&#39;Severity&#39;] = tmp1[&#39;Severity&#39;].values tmp2[&#39;Surface area&#39;] = tmp1[&#39;Side&#39;].values tmp2[&#39;Layer&#39;] = tmp1[&#39;Layer&#39;].values tmp2[&#39;maxRPM&#39;] = 17.3 X_raw = X_raw.append(tmp2, ignore_index=True) ##### WF6 Märkische Heide 2019 ##### folder_name = &#39;Märkische Heide 2019/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].unique(): tmp = df[df[&#39;Turbine&#39;]==wtg] for blade in tmp[&#39;Blade&#39;].unique(): tmp1 = tmp[tmp[&#39;Blade&#39;]==blade] tmp1 = tmp1.drop_duplicates(subset=[&#39;DefectId&#39;]) tmp2 = pd.DataFrame() tmp2[&#39;WTG ID&#39;] = tmp1[&#39;Turbine&#39;].values.astype(&#39;int32&#39;) tmp2[&#39;year&#39;] = &#39;2019&#39; tmp2[&#39;WF&#39;] = &#39;Märkische Heide&#39; tmp2[&#39;Blade&#39;] = tmp1[&#39;Blade&#39;].values.astype(&#39;int32&#39;) tmp2[&#39;Vertical distance&#39;] = tmp1[&#39;Vertical_Distance&#39;].values tmp2[&#39;Type&#39;] = tmp1[&#39;DefectType&#39;].values tmp2[&#39;Severity&#39;] = tmp1[&#39;Severity&#39;].values tmp2[&#39;Surface area&#39;] = tmp1[&#39;Side&#39;].values tmp2[&#39;Layer&#39;] = tmp1[&#39;Layer&#39;].values tmp2[&#39;maxRPM&#39;] = 17.3 X_raw = X_raw.append(tmp2, ignore_index=True) ##### WF7 Princess Amalia 2018 ##### folder_name = &#39;Princess Amalia 2018/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) tmp = pd.DataFrame() tmp[&#39;Blade&#39;] = df[&#39;Blade&#39;].values tmp[&#39;WTG ID&#39;] = &#39;WTG&#39;+file[12:14] tmp[&#39;year&#39;] = &#39;2018&#39; tmp[&#39;WF&#39;] = &#39;Princess Amalia&#39; tmp[&#39;Vertical distance&#39;] = df[&#39;Vertical_Distance&#39;].values tmp[&#39;Type&#39;] = df[&#39;DefectType&#39;].values tmp[&#39;Severity&#39;] = df[&#39;Severity&#39;].values tmp[&#39;Surface area&#39;] = df[&#39;Side&#39;].values tmp[&#39;Layer&#39;] = df[&#39;Layer&#39;].values tmp[&#39;maxRPM&#39;] = 19 X_raw = X_raw.append(tmp, ignore_index=True) ##### WF7 Princess Amalia 2019 ##### folder_name = &#39;Princess Amalia 2019/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].unique(): tmp = df[df[&#39;Turbine&#39;]==wtg] for blade in tmp[&#39;Blade&#39;].unique(): tmp1 = tmp[tmp[&#39;Blade&#39;]==blade] tmp1 = tmp1.drop_duplicates(subset=[&#39;Defect Id&#39;]) tmp2 = pd.DataFrame() tmp2[&#39;WTG ID&#39;] = tmp1[&#39;Turbine&#39;].values tmp2[&#39;year&#39;] = &#39;2019&#39; tmp2[&#39;WF&#39;] = &#39;Princess Amalia&#39; tmp2[&#39;Blade&#39;] = tmp1[&#39;Blade&#39;].values tmp2[&#39;Vertical distance&#39;] = tmp1[&#39;Vertical Distance&#39;].values tmp2[&#39;Type&#39;] = tmp1[&#39;Defect Type&#39;].values tmp2[&#39;Severity&#39;] = tmp1[&#39;Severity&#39;].values tmp2[&#39;Surface area&#39;] = tmp1[&#39;Surface Area&#39;].values tmp2[&#39;Layer&#39;] = tmp1[&#39;Layer&#39;].values tmp2[&#39;maxRPM&#39;] = 19 X_raw = X_raw.append(tmp2, ignore_index=True) ##### WF7 Princess Amalia 2020 ##### folder_name = &#39;Princess Amalia 2020/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;TurbineName&#39;].unique(): tmp = df[df[&#39;TurbineName&#39;]==wtg] for blade in tmp[&#39;Blade&#39;].unique(): tmp1 = tmp[tmp[&#39;Blade&#39;]==blade] tmp1 = tmp1.drop_duplicates(subset=[&#39;ReportDefectId&#39;]) tmp2 = pd.DataFrame() tmp2[&#39;WTG ID&#39;] = tmp1[&#39;TurbineName&#39;].values tmp2[&#39;year&#39;] = &#39;2020&#39; tmp2[&#39;WF&#39;] = &#39;Princess Amalia&#39; tmp2[&#39;Blade&#39;] = tmp1[&#39;Blade&#39;].values tmp2[&#39;Vertical distance&#39;] = tmp1[&#39;DistanceFromHub&#39;].values tmp2[&#39;Type&#39;] = tmp1[&#39;DefectType&#39;].values tmp2[&#39;Severity&#39;] = tmp1[&#39;Severity&#39;].values tmp2[&#39;Surface area&#39;] = tmp1[&#39;Surface&#39;].values tmp2[&#39;Layer&#39;] = tmp1[&#39;Layer&#39;].values tmp2[&#39;maxRPM&#39;] = 19 X_raw = X_raw.append(tmp2, ignore_index=True) ##### WF8 Solberg 2019 ##### folder_name = &#39;Solberg 2019/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].values: for b in [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]: cols = { &#39;WTG ID&#39;: str(wtg), &#39;year&#39;: &#39;2019&#39;, &#39;WF&#39;: &#39;Solberg&#39;, &#39;Blade&#39;: b, ##### NB DUMMY DEFECTS!!!! ##### &#39;Vertical distance&#39;: 0, &#39;Type&#39;: &#39;DUMMY&#39;, &#39;Severity&#39;: 0, &#39;Surface area&#39;: &#39;DUMMY&#39;, &#39;Layer&#39;: &#39;DUMMY&#39;, &#39;maxRPM&#39;: 0, } X_raw = X_raw.append(cols, ignore_index=True) ##### WF8 Solberg 2020 ##### folder_name = &#39;Solberg 2020/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].values: for b in [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]: cols = { &#39;WTG ID&#39;: str(wtg), &#39;year&#39;: &#39;2020&#39;, &#39;WF&#39;: &#39;Solberg&#39;, &#39;Blade&#39;: b, ##### NB DUMMY DEFECTS!!!! ##### &#39;Vertical distance&#39;: 0, &#39;Type&#39;: &#39;DUMMY&#39;, &#39;Severity&#39;: 0, &#39;Surface area&#39;: &#39;DUMMY&#39;, &#39;Layer&#39;: &#39;DUMMY&#39;, &#39;maxRPM&#39;: 0, } X_raw = X_raw.append(cols, ignore_index=True) ##### WF9 Camster 2018 ##### folder_name = &#39;Camster 2018/&#39; fnames = os.listdir(path_name+folder_name) for file in fnames: df = pd.read_excel(path_name+folder_name+file) for wtg in df[&#39;Turbine&#39;].unique(): tmp = df[df[&#39;Turbine&#39;]==wtg] for blade in tmp[&#39;Blade&#39;].unique(): tmp1 = tmp[tmp[&#39;Blade&#39;]==blade] tmp1 = tmp1.drop_duplicates(subset=[&#39;DefectId&#39;]) tmp2 = pd.DataFrame() tmp2[&#39;WTG ID&#39;] = tmp1[&#39;Turbine&#39;].values tmp2[&#39;year&#39;] = &#39;2018&#39; tmp2[&#39;WF&#39;] = &#39;Camster&#39; tmp2[&#39;Blade&#39;] = tmp1[&#39;Blade&#39;].values tmp2[&#39;Vertical distance&#39;] = tmp1[&#39;Vertical_Distance&#39;].values tmp2[&#39;Type&#39;] = tmp1[&#39;DefectType&#39;].values tmp2[&#39;Severity&#39;] = tmp1[&#39;Severity&#39;].values tmp2[&#39;Surface area&#39;] = tmp1[&#39;Side&#39;].values tmp2[&#39;Layer&#39;] = tmp1[&#39;Layer&#39;].values tmp2[&#39;maxRPM&#39;] = 19 X_raw = X_raw.append(tmp2, ignore_index=True) # calculate potential radial speed for each defect X_raw[&#39;Potential speed&#39;] = X_raw[&#39;Vertical distance&#39;]*2*np.pi*X_raw[&#39;maxRPM&#39;]/60 # drop MH 2016 4XXXXX turbines X_raw = X_raw[~X_raw[&#39;WTG ID&#39;].isin([41273, 41274, 41275])] . Looping through all individual blades to get a dataframe with all the unique blades that has been inspected, including those with zero defects observed. This df will be used later. In addition, Princess Amalia had some blade inspections that have been deemed invalid or repaired and these will be removed . X_unique_blades = pd.DataFrame() for idx, row in X_raw[[&#39;WF&#39;, &#39;year&#39;, &#39;WTG ID&#39;, &#39;Blade&#39;]].value_counts().iteritems(): cols = { &#39;WF&#39;: idx[0], &#39;year&#39;: idx[1], &#39;WTG&#39;: idx[2], &#39;Blade&#39;: idx[3] } X_unique_blades = X_unique_blades.append(cols, ignore_index=True) # Removing repaired turbines PA_repairs = pd.read_excel(&#39;data/project_data.xlsx&#39;, sheet_name=&#39;PA repairs&#39;) for i in range(len(PA_repairs)): tmp = PA_repairs.iloc[i] if ((tmp == &#39;Repaired&#39;) | (tmp == &#39;Difference in assessment&#39;)).any(): cond = ( (X_unique_blades[&#39;WF&#39;] == &#39;Princess Amalia&#39;) &amp; (X_unique_blades[&#39;WTG&#39;] == tmp[&#39;Turbine&#39;]) &amp; (X_unique_blades[&#39;Blade&#39;] == tmp[&#39;blade&#39;]) ) X_unique_blades = X_unique_blades.drop(X_unique_blades[cond].index) print(&#39;In total&#39;, len(X_unique_blades), &#39;blades have been inspected&#39;) print(&#39;The unique blades that have been inspected are:&#39;) #X_unique_blades . In total 575 blades have been inspected The unique blades that have been inspected are: . Now we apply a filter to only get defects which fulfills: . No invalid defects (nan) | Only leading edge defects | Only defect types: voids, chipping, peeling and erosion | Only defects exposed to a potential impact velocity greater than 65 m/s (corresponds roughly to outer 20% of blade) | . conditions = ( (X_raw[&#39;Surface area&#39;]==&#39;LE&#39;) &amp; # only looking at leading edge defects (X_raw[&#39;Potential speed&#39;]&gt;0) &amp; # potential speed greater or equal to 65 m/s (corresponds roughly to outer 20% of blade) (X_raw[&#39;Type&#39;].isin([&#39;Voids&#39;, &#39;Chipping&#39;, &#39;Peeling&#39;, &#39;Erosion&#39;])) # only looking at the four defect types ) X_filt = X_raw[conditions] # dropping nan X_filt = X_filt.dropna(subset=[&#39;Type&#39;, &#39;Severity&#39;], inplace=False) # Converting roman severity into numerical X_filt = X_filt.replace([&#39;I&#39;, &#39;II&#39;, &#39;III&#39;, &#39;IV&#39;, &#39;V&#39;], [1, 2, 3, 4, 5]) # convert severity to int X_filt = X_filt.astype({&#39;Severity&#39;: int}) # replace type with numerical value X_filt = X_filt.replace([&#39;Voids&#39;,&#39;Chipping&#39;,&#39;Peeling&#39;,&#39;Erosion&#39;], [1, 2, 3, 4]) # drop defects that have type &lt;4 and severity &gt;3 conditions = ( (X_filt[&#39;Type&#39;]&lt;4) &amp; (X_filt[&#39;Severity&#39;]&gt;3) ) X_filt = X_filt[~conditions] print(&#39;Simple filter, we are now going from&#39;, len(X_raw), &#39;to&#39;, len(X_filt), &#39;defects&#39;) HF.plot_marginal_distribution(X_filt) X_filt2 = X_filt[X_filt[&#39;Potential speed&#39;]&gt;X_filt[&#39;Potential speed&#39;].max()*0.8] print(&#39;Filtering for potential speed &gt;&#39;,round(X_filt[&#39;Potential speed&#39;].max()*0.8, 2), &#39;we are now going from&#39;, len(X_filt), &#39;to&#39;, len(X_filt2), &#39;defects&#39;) HF.plot_marginal_distribution(X_filt2) #X_filt = X_filt2 . Simple filter, we are now going from 13966 to 9929 defects . Filtering for potential speed &gt; 65.22 we are now going from 9929 to 2491 defects . Now we add a weigth to each defect based on the weighting scheme shown below: . weights = pd.read_excel(&#39;data/project_data.xlsx&#39;, usecols=[0,1,2,3], sheet_name=&#39;damage weights&#39;) D = [] for i in range(len(X_filt)): tmp = X_filt.iloc[i] cond = ( (weights[&#39;Type&#39;]==tmp[&#39;Type&#39;])&amp; (weights[&#39;Severity&#39;]==tmp[&#39;Severity&#39;]) ) D.append(weights[cond][&#39;Weight&#39;].values[0]) X_filt[&#39;Weight&#39;] = np.array(D) X_filt.to_pickle(&#39;data/filtered_defects.pkl&#39;) . Now we will add a bunch of different defect statistics to evaluate which damage metric is better to use and matches the weather data the best: . X_wf = pd.DataFrame() for wf in X_unique_blades[&#39;WF&#39;].unique(): for year in X_unique_blades[X_unique_blades[&#39;WF&#39;]==wf][&#39;year&#39;].unique(): cond = ( (X_unique_blades[&#39;WF&#39;]==wf) &amp; (X_unique_blades[&#39;year&#39;]==year) ) df = X_unique_blades[cond] cols = { &#39;WF&#39;: wf, &#39;year&#39;: year, &#39;total turbines&#39;: X_info[X_info[&#39;Name&#39;]==wf][&#39;No WTG&#39;].values[0], &#39;inspected turbines&#39;: len(df[&#39;WTG&#39;].unique()), &#39;total blades&#39;: X_info[X_info[&#39;Name&#39;]==wf][&#39;No blades&#39;].values[0], &#39;inspected blades&#39;: len(df), } X_wf = X_wf.append(cols, ignore_index=True) X_wf = X_wf[[&#39;WF&#39;, &#39;year&#39;, &#39;total turbines&#39;, &#39;inspected turbines&#39;, &#39;total blades&#39;, &#39;inspected blades&#39;]] # add directly wf inspection mean and std of weight, type and severity tmp = pd.DataFrame() for idx, row in X_wf.iterrows(): cond = ( (X_filt[&#39;WF&#39;]==row[&#39;WF&#39;]) &amp; (X_filt[&#39;year&#39;]==row[&#39;year&#39;]) ) df = X_filt[cond] cols = { &#39;defects&#39;: len(df), #&#39;01_mu_W&#39;: df[&#39;Weight&#39;].mean(), &#39;01_mu_T&#39;: df[&#39;Type&#39;].mean(), &#39;01_mu_S&#39;: df[&#39;Severity&#39;].mean(), #&#39;01_sd_W&#39;: df[&#39;Weight&#39;].std(), &#39;01_sd_T&#39;: df[&#39;Type&#39;].std(), &#39;01_sd_S&#39;: df[&#39;Severity&#39;].std(), } tmp = tmp.append(cols, ignore_index=True) tmp = tmp.fillna(0) X_wf = pd.concat([X_wf, tmp], axis=1) # add stepwise wf inspection mean and std of weight, type and severity ###### go from per-defect to per-blade X_blade = pd.DataFrame() for index, row in X_unique_blades.iterrows(): condition = ( (X_filt[&#39;WF&#39;]==row[&#39;WF&#39;]) &amp; (X_filt[&#39;year&#39;]==row[&#39;year&#39;]) &amp; (X_filt[&#39;WTG ID&#39;]==row[&#39;WTG&#39;]) &amp; (X_filt[&#39;Blade&#39;]==row[&#39;Blade&#39;]) ) tmp = X_filt[condition] cols = { &#39;WF&#39;: row[&#39;WF&#39;], &#39;year&#39;: row[&#39;year&#39;], &#39;WTG&#39;: row[&#39;WTG&#39;], &#39;Blade&#39;: row[&#39;Blade&#39;], &#39;mu_W&#39;: tmp[&#39;Weight&#39;].mean(), &#39;mu_T&#39;: tmp[&#39;Type&#39;].mean(), &#39;mu_S&#39;: tmp[&#39;Severity&#39;].mean(), &#39;sd_W&#39;: tmp[&#39;Weight&#39;].std(), &#39;sd_T&#39;: tmp[&#39;Type&#39;].std(), &#39;sd_S&#39;: tmp[&#39;Severity&#39;].std(), &#39;max_W&#39;: tmp[&#39;Weight&#39;].max(), } X_blade = X_blade.append(cols, ignore_index=True) &quot;&quot;&quot; ###### go from per-blade to per-wtg X_wtg = pd.DataFrame() for idx, row in X_wf.iterrows(): cond = ( (X_blade[&#39;WF&#39;]==row[&#39;WF&#39;]) &amp; (X_blade[&#39;year&#39;]==row[&#39;year&#39;]) ) df = X_blade[cond] for wtg in df[&#39;WTG&#39;].unique(): tmp = df[df[&#39;WTG&#39;]==wtg] cols = { &#39;WF&#39;: row[&#39;WF&#39;], &#39;year&#39;: row[&#39;year&#39;], &#39;WTG&#39;: wtg, &#39;mu_W&#39;: tmp[&#39;mu_W&#39;].mean(), &#39;mu_T&#39;: tmp[&#39;mu_T&#39;].mean(), &#39;mu_S&#39;: tmp[&#39;mu_S&#39;].mean(), &#39;mu_max_W&#39;: tmp[&#39;max_W&#39;].mean(), } X_wtg = X_wtg.append(cols, ignore_index=True) ###### go from per-wtg to per-wf X = pd.DataFrame() for idx, row in X_wf.iterrows(): cond = ( (X_wtg[&#39;WF&#39;]==row[&#39;WF&#39;]) &amp; (X_wtg[&#39;year&#39;]==row[&#39;year&#39;]) ) df = X_wtg[cond] cols = { &#39;02_mu_W&#39;: df[&#39;mu_W&#39;].mean(), &#39;02_mu_max_W&#39;: df[&#39;mu_max_W&#39;].mean(), #&#39;02_mu_T&#39;: df[&#39;mu_T&#39;].mean(), #&#39;02_mu_S&#39;: df[&#39;mu_S&#39;].mean(), } X = X.append(cols, ignore_index=True) X = X.fillna(0) X_wf = pd.concat([X_wf, X], axis=1) &quot;&quot;&quot; X = pd.DataFrame() for idx, row in X_wf.iterrows(): cond = ( (X_blade[&#39;WF&#39;]==row[&#39;WF&#39;]) &amp; (X_blade[&#39;year&#39;]==row[&#39;year&#39;]) ) df = X_blade[cond] cols = { &#39;02_mu_W&#39;: df[&#39;mu_W&#39;].mean(), &#39;02_mu_max_W&#39;: df[&#39;max_W&#39;].mean(), #&#39;02_mu_T&#39;: df[&#39;mu_T&#39;].mean(), #&#39;02_mu_S&#39;: df[&#39;mu_S&#39;].mean(), } X = X.append(cols, ignore_index=True) X = X.fillna(0) X_wf = pd.concat([X_wf, X], axis=1) X_wf . WF year total turbines inspected turbines total blades inspected blades defects 01_mu_T 01_mu_S 01_sd_T 01_sd_S 02_mu_W 02_mu_max_W . 0 Tween Bridge | 2017 | 22.0 | 7.0 | 66.0 | 20.0 | 253.0 | 1.355731 | 1.612648 | 0.886370 | 0.672709 | 0.083276 | 0.207647 | . 1 Løgtved | 2019 | 3.0 | 1.0 | 9.0 | 2.0 | 25.0 | 1.200000 | 1.240000 | 0.408248 | 0.522813 | 0.052675 | 0.200000 | . 2 Løgtved | 2018 | 3.0 | 1.0 | 9.0 | 3.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 3 Princess Amalia | 2018 | 60.0 | 10.0 | 180.0 | 19.0 | 262.0 | 1.675573 | 2.583969 | 0.771682 | 0.611677 | 0.280564 | 0.468421 | . 4 Princess Amalia | 2019 | 60.0 | 49.0 | 180.0 | 138.0 | 957.0 | 1.824451 | 2.175549 | 0.908184 | 0.894256 | 0.290020 | 0.502463 | . 5 Princess Amalia | 2020 | 60.0 | 50.0 | 180.0 | 141.0 | 492.0 | 2.335366 | 2.538618 | 1.006764 | 0.728062 | 0.397662 | 0.564104 | . 6 Camster | 2018 | 25.0 | 24.0 | 75.0 | 72.0 | 441.0 | 1.750567 | 2.455782 | 1.200911 | 0.728304 | 0.267701 | 0.686806 | . 7 Brorstrup | 2019 | 2.0 | 2.0 | 6.0 | 6.0 | 17.0 | 3.000000 | 2.941176 | 0.612372 | 0.242536 | 0.530556 | 0.633333 | . 8 Märkische Heide | 2019 | 8.0 | 8.0 | 24.0 | 23.0 | 30.0 | 3.066667 | 2.366667 | 1.048261 | 0.614948 | 0.472619 | 0.538095 | . 9 Märkische Heide | 2016 | 8.0 | 7.0 | 24.0 | 19.0 | 14.0 | 2.142857 | 2.214286 | 0.949262 | 0.578934 | 0.282273 | 0.295455 | . 10 Solberg | 2020 | 22.0 | 22.0 | 66.0 | 66.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 11 Solberg | 2019 | 22.0 | 22.0 | 66.0 | 66.0 | 0.0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . Convert from per-inspection into per-sequence: . W0, dW, W02, dW2 = [], [], [], [] for idx, row in X_info.iterrows(): tmp = X_wf[X_wf[&#39;WF&#39;]==row[&#39;Name&#39;]] for i in list(combinations(row[&#39;Inspections&#39;].split(&#39;,&#39;), 2)): #print(i[0][:4], i[1][:4], tmp[&#39;year&#39;].unique()) # if the period is between two inspections if i[0][:4] in tmp[&#39;year&#39;].unique(): #print(&#39;totals trick&#39;) W0.append(tmp[tmp[&#39;year&#39;]==i[0][:4]][&#39;02_mu_W&#39;].values[0]) dW.append(tmp[tmp[&#39;year&#39;]==i[1][:4]][&#39;02_mu_W&#39;].values[0] - tmp[tmp[&#39;year&#39;]==i[0][:4]][&#39;02_mu_W&#39;].values[0]) W02.append(tmp[tmp[&#39;year&#39;]==i[0][:4]][&#39;02_mu_max_W&#39;].values[0]) dW2.append(tmp[tmp[&#39;year&#39;]==i[1][:4]][&#39;02_mu_max_W&#39;].values[0] - tmp[tmp[&#39;year&#39;]==i[0][:4]][&#39;02_mu_max_W&#39;].values[0]) # else the period is between commission and inspection else: #print(&#39;commission date&#39;) W0.append(0) dW.append(tmp[tmp[&#39;year&#39;]==i[1][:4]][&#39;02_mu_W&#39;].values[0]) W02.append(0) dW2.append(tmp[tmp[&#39;year&#39;]==i[1][:4]][&#39;02_mu_max_W&#39;].values[0]) X_tt[&#39;W0&#39;] = np.array(W0) X_tt[&#39;dW&#39;] = np.array(dW) X_tt[&#39;W&#39;] = X_tt[&#39;W0&#39;] + X_tt[&#39;dW&#39;] X_tt[&#39;W02&#39;] = np.array(W02) X_tt[&#39;dW2&#39;] = np.array(dW2) X_tt[&#39;W2&#39;] = X_tt[&#39;W02&#39;] + X_tt[&#39;dW2&#39;] X_tt.to_csv(&#39;data/processed_damage_NEWA.csv&#39;, index=False) X_tt . WF start date end date months t_scaler prec_sum ws_sum tke_sum prec_mean ws_mean tke_mean impingement da_inc W0 dW W W02 dW2 W2 . 0 Brorstrup | 2012-01-01 | 2019-05-01 | 88.0 | 1.26 | 6433.538273 | 461307.060595 | 58078.544265 | 0.128985 | 9.248708 | 1.164412 | 96.005915 | 0.786705 | 0.000000 | 0.530556 | 0.530556 | 0.000000 | 0.633333 | 0.633333 | . 1 Løgtved | 2016-06-01 | 2018-05-01 | 23.0 | 1.00 | 1237.562318 | 143919.129525 | 9064.112082 | 0.075319 | 8.759000 | 0.551647 | 20.541667 | 0.116657 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 2 Løgtved | 2016-06-01 | 2019-05-01 | 35.0 | 1.00 | 1758.937686 | 210339.803826 | 16799.441743 | 0.070147 | 8.388427 | 0.669968 | 29.199881 | 0.156682 | 0.000000 | 0.052675 | 0.052675 | 0.000000 | 0.200000 | 0.200000 | . 3 Løgtved | 2018-05-01 | 2019-05-01 | 12.0 | 1.00 | 521.375368 | 66430.016633 | 7735.568618 | 0.060309 | 7.684212 | 0.894803 | 8.658213 | 0.040025 | 0.000000 | 0.052675 | 0.052675 | 0.000000 | 0.200000 | 0.200000 | . 4 Tween Bridge | 2012-03-01 | 2017-09-01 | 66.0 | 1.33 | 3523.168521 | 300681.228387 | 35457.630242 | 0.099999 | 8.534322 | 1.006404 | 52.078595 | 0.299888 | 0.000000 | 0.083276 | 0.083276 | 0.000000 | 0.207647 | 0.207647 | . 5 Märkische Heide | 2009-05-01 | 2016-05-01 | 84.0 | 2.49 | 4106.410059 | 415622.383524 | 41458.311973 | 0.170809 | 17.288066 | 1.724484 | 62.552778 | 0.585163 | 0.000000 | 0.282273 | 0.282273 | 0.000000 | 0.295455 | 0.295455 | . 6 Märkische Heide | 2009-05-01 | 2019-05-01 | 120.0 | 1.72 | 5866.043133 | 592357.076532 | 57021.883915 | 0.117617 | 11.877072 | 1.143319 | 90.353759 | 0.829741 | 0.000000 | 0.472619 | 0.472619 | 0.000000 | 0.538095 | 0.538095 | . 7 Märkische Heide | 2016-05-01 | 2019-05-01 | 36.0 | 1.00 | 1759.152528 | 177269.203133 | 16480.181809 | 0.068094 | 6.861857 | 0.637926 | 27.376590 | 0.247092 | 0.282273 | 0.190346 | 0.472619 | 0.295455 | 0.242641 | 0.538095 | . 8 Princess Amalia | 2013-05-01 | 2018-05-01 | 60.0 | 1.04 | 4119.002753 | 383803.786882 | 20797.127892 | 0.100434 | 9.358329 | 0.507099 | 71.582376 | 1.226624 | 0.000000 | 0.280564 | 0.280564 | 0.000000 | 0.468421 | 0.468421 | . 9 Princess Amalia | 2013-05-01 | 2019-05-01 | 72.0 | 1.03 | 4711.439212 | 460751.225546 | 23564.837998 | 0.094991 | 9.289527 | 0.475107 | 82.377549 | 1.416901 | 0.000000 | 0.290020 | 0.290020 | 0.000000 | 0.502463 | 0.502463 | . 10 Princess Amalia | 2013-05-01 | 2020-05-01 | 84.0 | 1.03 | 5620.196770 | 544218.153967 | 27057.615912 | 0.096527 | 9.346973 | 0.464716 | 98.577428 | 1.734760 | 0.000000 | 0.397662 | 0.397662 | 0.000000 | 0.564104 | 0.564104 | . 11 Princess Amalia | 2018-05-01 | 2019-05-01 | 12.0 | 1.00 | 600.571168 | 76958.926446 | 2811.923707 | 0.069931 | 8.961216 | 0.327425 | 10.923373 | 0.192670 | 0.280564 | 0.009456 | 0.290020 | 0.468421 | 0.034042 | 0.502463 | . 12 Princess Amalia | 2018-05-01 | 2020-05-01 | 24.0 | 1.00 | 1506.119020 | 160254.448022 | 6316.396732 | 0.087499 | 9.310082 | 0.366955 | 27.058942 | 0.508389 | 0.280564 | 0.117098 | 0.397662 | 0.468421 | 0.095683 | 0.564104 | . 13 Princess Amalia | 2019-05-01 | 2020-05-01 | 12.0 | 1.00 | 905.548341 | 83297.437347 | 3504.591158 | 0.104979 | 9.656554 | 0.406282 | 16.135584 | 0.315719 | 0.290020 | 0.107642 | 0.397662 | 0.502463 | 0.061642 | 0.564104 | . 14 Solberg | 2018-01-01 | 2019-05-01 | 16.0 | 1.00 | 311.255188 | 78248.213688 | 8899.638372 | 0.028548 | 7.176760 | 0.816256 | 4.644573 | 0.063678 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 15 Solberg | 2018-01-01 | 2020-05-01 | 28.0 | 1.00 | 774.064942 | 142411.173071 | 16564.841328 | 0.039319 | 7.233767 | 0.841410 | 11.240074 | 0.172157 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 16 Solberg | 2019-05-01 | 2020-05-01 | 12.0 | 1.00 | 462.809754 | 64172.139547 | 7666.317368 | 0.052682 | 7.304740 | 0.872660 | 6.595501 | 0.108478 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 17 Camster | 2013-07-01 | 2018-05-01 | 58.0 | 1.05 | 4508.322322 | 331497.860727 | 46444.603144 | 0.112396 | 8.264512 | 1.157902 | 71.520520 | 0.675748 | 0.000000 | 0.267701 | 0.267701 | 0.000000 | 0.686806 | 0.686806 | . sns.pairplot(X_tt[[&#39;W0&#39;,&#39;W02&#39;,&#39;impingement&#39;,&#39;dW&#39;,&#39;W&#39;,&#39;dW2&#39;,&#39;W2&#39;]]) plt.show() . print(&#39;PA 2018&#39;) HF.plot_marginal_distribution(X_filt[((X_filt[&#39;WF&#39;]==&#39;Princess Amalia&#39;)&amp;(X_filt[&#39;year&#39;]==&#39;2018&#39;))]) print(&#39;PA 2019&#39;) HF.plot_marginal_distribution(X_filt[((X_filt[&#39;WF&#39;]==&#39;Princess Amalia&#39;)&amp;(X_filt[&#39;year&#39;]==&#39;2019&#39;))]) print(&#39;PA 2020&#39;) HF.plot_marginal_distribution(X_filt[((X_filt[&#39;WF&#39;]==&#39;Princess Amalia&#39;)&amp;(X_filt[&#39;year&#39;]==&#39;2020&#39;))]) print(&#39;MH 2016&#39;) HF.plot_marginal_distribution(X_filt[((X_filt[&#39;WF&#39;]==&#39;Märkische Heide&#39;)&amp;(X_filt[&#39;year&#39;]==&#39;2016&#39;))]) print(&#39;MH 2019&#39;) HF.plot_marginal_distribution(X_filt[((X_filt[&#39;WF&#39;]==&#39;Märkische Heide&#39;)&amp;(X_filt[&#39;year&#39;]==&#39;2019&#39;))]) . PA 2018 . PA 2019 . PA 2020 . MH 2016 . MH 2019 . Decoding . We are now making a decoding model that takes a . X_decode = X_wf[[&#39;01_mu_W&#39;, &#39;01_mu_T&#39;, &#39;01_mu_S&#39;, &#39;01_sd_T&#39;, &#39;01_sd_S&#39;]] # add dummy row new_row = { &#39;01_mu_W&#39;: 1, &#39;01_mu_T&#39;: 4, &#39;01_mu_S&#39;: 4, &#39;01_sd_T&#39;: 0, &#39;01_sd_S&#39;: 0, } X_decode = X_decode.append(new_row, ignore_index=True) x = X_decode[&#39;01_mu_W&#39;].values.ravel() y = X_decode[[&#39;01_mu_T&#39;, &#39;01_mu_S&#39;, &#39;01_sd_T&#39;, &#39;01_sd_S&#39;]].values interp_func = interpolate.interp1d(x, y, kind=&#39;slinear&#39;, axis=0) stat = np.empty((50, 4)) x_interp = np.linspace(0, 1) idx = 0 for D in x_interp: stat[idx, :] = interp_func(D) idx += 1 font = {&#39;family&#39; : &#39;serif&#39;, &#39;size&#39; : 12} plt.rc(&#39;font&#39;, **font) fig = plt.figure(figsize=[14,10]) # [14,8.2] = full size # Plot damage vs starting damage (fixed date range) ax = plt.subplot(221) # 1 rows x 2 columns subplot no. 1 plt.scatter(X_wf[&#39;01_mu_W&#39;], X_wf[&#39;01_mu_T&#39;], s=50) plt.plot(x_interp, stat[:,0], linestyle=&#39;dashed&#39;, color=&#39;r&#39;) ax.grid(b=True, which=&#39;major&#39;) ax.set_ylabel(r&#39;mean defect type [-]&#39;) ax.set_xlabel(r&#39;encoded damage [-]&#39;) ax = plt.subplot(222) # 1 rows x 2 columns subplot no. 1 plt.scatter(X_wf[&#39;01_mu_W&#39;], X_wf[&#39;01_sd_T&#39;], s=50) plt.plot(x_interp, stat[:,2], linestyle=&#39;dashed&#39;, color=&#39;r&#39;) ax.grid(b=True, which=&#39;major&#39;) ax.set_ylabel(r&#39;std of defect type [-]&#39;) ax.set_xlabel(r&#39;encoded damage [-]&#39;) ax = plt.subplot(223) # 1 rows x 2 columns subplot no. 1 plt.scatter(X_wf[&#39;01_mu_W&#39;], X_wf[&#39;01_mu_S&#39;], s=50) plt.plot(x_interp, stat[:,1], linestyle=&#39;dashed&#39;, color=&#39;r&#39;) ax.grid(b=True, which=&#39;major&#39;) ax.set_ylabel(r&#39;mean defect severity [-]&#39;) ax.set_xlabel(r&#39;encoded damage [-]&#39;) ax = plt.subplot(224) # 1 rows x 2 columns subplot no. 1 plt.scatter(X_wf[&#39;01_mu_W&#39;], X_wf[&#39;01_sd_S&#39;], s=50) plt.plot(x_interp, stat[:,3], linestyle=&#39;dashed&#39;, color=&#39;r&#39;) ax.grid(b=True, which=&#39;major&#39;) ax.set_ylabel(r&#39;std of defect severity [-]&#39;) ax.set_xlabel(r&#39;encoded damage [-]&#39;) plt.show() . KeyError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_16948/1025727928.py in &lt;module&gt; -&gt; 1 X_decode = X_wf[[&#39;01_mu_W&#39;, &#39;01_mu_T&#39;, &#39;01_mu_S&#39;, &#39;01_sd_T&#39;, &#39;01_sd_S&#39;]] 2 # add dummy row 3 new_row = { 4 &#39;01_mu_W&#39;: 1, 5 &#39;01_mu_T&#39;: 4, ~ Anaconda3 envs ML-env lib site-packages pandas core frame.py in __getitem__(self, key) 3462 if is_iterator(key): 3463 key = list(key) -&gt; 3464 indexer = self.loc._get_listlike_indexer(key, axis=1)[1] 3465 3466 # take() does not accept boolean indexers ~ Anaconda3 envs ML-env lib site-packages pandas core indexing.py in _get_listlike_indexer(self, key, axis) 1312 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr) 1313 -&gt; 1314 self._validate_read_indexer(keyarr, indexer, axis) 1315 1316 if needs_i8_conversion(ax.dtype) or isinstance( ~ Anaconda3 envs ML-env lib site-packages pandas core indexing.py in _validate_read_indexer(self, key, indexer, axis) 1375 1376 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()) -&gt; 1377 raise KeyError(f&#34;{not_found} not in index&#34;) 1378 1379 KeyError: &#34;[&#39;01_mu_W&#39;] not in index&#34; . def expected_distribution(da, interp_func): # DEFECT TYPE Dbins = np.array([-1000, 0.5, 1.5, 2.5, 3.5, 1000]) Sbins = np.array([-1000, 0.5, 1.5, 2.5, 1000]) Dm = np.arange(0, 5) Sm = np.arange(0, 4) m1, m2, s1, s2 = interp_func(da) df = pd.DataFrame() for i in range(len(Dbins)-1): D = stats.norm.cdf(Dbins[i+1], m1, s1) - stats.norm.cdf(Dbins[i], m1, s1) for j in range(len(Sbins)-1): S = stats.norm.cdf(Sbins[j+1], m2, s2) - stats.norm.cdf(Sbins[j], m2, s2) cols = { &#39;Type&#39;: Dm[i], &#39;Severity&#39;: Sm[j], &#39;P&#39;: (100*D*S/1) } df = df.append(cols, ignore_index=True) cond = (df[&#39;Severity&#39;]!=0) &amp; (df[&#39;Type&#39;]!=0) df = df[cond] df = df.pivot(&#39;Type&#39;, &#39;Severity&#39;, &#39;P&#39;) # Plot Heatmap font = {&#39;family&#39; : &#39;serif&#39;, &#39;size&#39; : 14} plt.rc(&#39;font&#39;, **font) fig = plt.figure(figsize=[6,6]) # [14,8.2] = full size ax = plt.subplot(111) ax = sns.heatmap(df, cmap=&#39;RdYlGn&#39;, square=True, annot=True, fmt=&#39;.0f&#39;, yticklabels=[&#39;Voids&#39;, &#39;Chipping&#39;, &#39;Peeling&#39;, &#39;Erosion&#39;], cbar_kws={&#39;label&#39;: &#39;Probability [%]&#39;}) ax.set_ylabel(&#39;&#39;) ax.tick_params(axis=&#39;y&#39;, rotation=0) ax.invert_yaxis() plt.title(&#39;Predicted distribution&#39;) plt.show() . expected_distribution(0.37, interp_func) . df = pd.read_excel(&#39;data/project_data.xlsx&#39;, sheet_name=&#39;damage weights&#39;) df = df[[&#39;Defect&#39;, &#39;Type&#39;, &#39;Severity&#39;, &#39;Weight&#39;]] # specify model prediction mu = 0.5 sd = 0.3 # calculate distance to each weight df[&#39;Distance&#39;] = df[&#39;Weight&#39;]-mu # Calculate weights based on a normal distribution df[&#39;Prob_norm&#39;] = stats.norm.pdf(df[&#39;Distance&#39;].values, loc=0, scale=sd) # scale df[&#39;Prob_norm&#39;] = 100*df[&#39;Prob_norm&#39;]/df[&#39;Prob_norm&#39;].sum() # Plot distribution weight = df.pivot(&#39;Type&#39;, &#39;Severity&#39;, &#39;Prob_norm&#39;) # Plot Heatmap font = {&#39;family&#39; : &#39;serif&#39;, &#39;size&#39; : 14} plt.rc(&#39;font&#39;, **font) fig = plt.figure(figsize=[6,6]) # [14,8.2] = full size ax = plt.subplot(111) ax = sns.heatmap(weight, cmap=&#39;RdYlGn&#39;, square=True, annot=True, fmt=&#39;.0f&#39;, yticklabels=[&#39;Voids&#39;, &#39;Chipping&#39;, &#39;Peeling&#39;, &#39;Erosion&#39;], cbar_kws={&#39;label&#39;: &#39;Weight&#39;}) ax.set_ylabel(&#39;&#39;) ax.tick_params(axis=&#39;y&#39;, rotation=0) ax.invert_yaxis() #plt.title(r&#39;Defect weighting scheme&#39;) plt.show() . X_dist = pd.DataFrame() for t in np.arange(1, 5): for s in np.arange(1, 4): cond = ( (X_filt[&#39;Type&#39;]==t) &amp; (X_filt[&#39;Severity&#39;]==s) ) #tmp = df[cond] cols = { &#39;Severity&#39;: s, &#39;Type&#39;: t, &#39;n&#39;: cond.sum(), } X_dist = X_dist.append(cols, ignore_index=True) X_dist[&#39;N&#39;] = 100*X_dist[&#39;n&#39;]/X_dist[&#39;n&#39;].sum() weight = X_dist.pivot(&#39;Type&#39;, &#39;Severity&#39;, &#39;N&#39;) # Plot Heatmap font = {&#39;family&#39; : &#39;serif&#39;, &#39;size&#39; : 14} plt.rc(&#39;font&#39;, **font) fig = plt.figure(figsize=[5,5]) # [14,8.2] = full size ax = plt.subplot(111) ax = sns.heatmap(weight, cmap=&#39;RdYlGn&#39;, square=True, annot=True, fmt=&#39;.1f&#39;, yticklabels=[&#39;Voids&#39;, &#39;Chipping&#39;, &#39;Peeling&#39;, &#39;Erosion&#39;], cbar_kws={&#39;label&#39;: &#39;%&#39;}) ax.set_ylabel(&#39;&#39;) ax.tick_params(axis=&#39;y&#39;, rotation=0) ax.invert_yaxis() plt.show() .",
            "url": "https://jensvmadsen.github.io/portfolio/2021/12/23/weather-data-analysis.html",
            "relUrl": "/2021/12/23/weather-data-analysis.html",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jensvmadsen.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jensvmadsen.github.io/portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Graduate wind engineer with a personal passion for machine learning and data driven analysis. Wide variety of skills within the field of wind energy including site assessment according to IEC standards , wind turbine sizing , grid connection and wind farm economics . Professional experience working with machine learning approaches for predicting leading erosion at DTU Wind Energy. . .",
          "url": "https://jensvmadsen.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "CV",
          "content": "Jens Visbech Madsen . jvima@dtu.dk | (+45) 93511409 | Copenhagen, Denmark | . PhD student at the Technical University of Denmark focusing on AI-approaches in the context of leading edge erosion. . Education . Technical University of Denmark, MSc Wind Energy 2018 - 2021 . AI | Aeroelastic simulations | . Technical University of Denmark, BSc Earth and Space Phyics and Engineering 2015 - 2018 . Weather and climate systems | Electrical systems | . Proffesional experience . PhD student, DTU Wind Energy OCT2021 - . Research assistant, DTU Wind Energy MAR2021 - OCT2021 . Student assistant, Ørsted JUL2018 - DEC2020 . Calibration assistant, Svend Ole Hansen A FEB2018 - DEC2020 .",
          "url": "https://jensvmadsen.github.io/portfolio/cv/",
          "relUrl": "/cv/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jensvmadsen.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}